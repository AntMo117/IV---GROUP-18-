
<h1> Machine Learning </h1>
<p>Machine Learning is understood as a branch of Artificial Intelligence and is usually divided in two categories: Strong and weak Artificial Intelligence. </p>

<p>The ultimate objective for machine learning is to create strong AI, which are algorithms with ability to improve and learn from its own experience. The machine would learn from new information acquired through its own algorithms, and, before the next prediction or decision, the machine updates its algorithm.</p>

<p>This high level of machine learning still a bit obscure to us, but industry leaders have been very keen on its development, resulting in a lot of progress in artificial intelligence in recent years.</p>

<p>Meanwhile, the use of weak AI has increased significantly in many industries. Weak AI is defined as algorithms trained trough data input to perform specify tasks. Differently from strong AI, weak AI does not have capacity to learn from its own experience and its capabilities will remain the same unless there is human interference. 
The performance enhancement for these algorithms is done by humans analysing new data and implementing new algorithms modelling in the machine. 
This technique is called reinforcement learning and it is different from the process that happens with strong AI, which is called unsupervised learning.</p>

<p>An example of Strong AI is the AlphaGo software. This computer program was developed to play the ancient Chinese board game called Go.
Go is considered one of the hardest board games because of the number of different possible strategies that can be used and created during a game. Therefore, there are endless ways to win a game and it creates the challenge to write a computer program capable of winning against an expert player.<br>
AlphaGo was trained with information from other games played by expert players. After it had enough information to play a game, the program played numerous rounds against itself and kept on learning new strategies from each of them. <br>
This is the reason that AlphaGo can be classified as a strong artificial intelligence machine. Even though it is a narrow intelligent program, it does have the capability to learn from its own information and update its algorithms to perform better. AlphaGo had its first win against an expert player in October 2015 and it is considered a milestone in the research field of artificial intelligence.</p>

<p>More commonly found are examples of weak AI, which are now being used in everyday life and being implemented by non-IT industries.</p>

<p>Voice assistants, such as Google Assistant or Siri from Apple, are trained machines that can answers to queries by voice activation. It is able to tell you what is next on your calendar, find a recipe for dinner or remind you of when to leave home so you are not late for an appointment. <br>

This kind of artificial intelligence is not able to outperform what it is trained for. For example, you can ask your virtual assistant to start playing your music playlist every day at 7pm and it will do as told. Although, even if you have the routine of playing music every day at 7pm, the machine will not learn from that and set a task for itself without you asking for it. This is a simple but useful example to clarify the difference between strong and weak artificial intelligence.</p>

<p>The tasks each machine with weak AI are trained to do are limited but it still able to make great impact in a lot of areas it is applied to.</p>

<p>For example, AI in healthcare. It is well known that machines trained to identify diseases through image analysis are used in Radiology and greatly contribute to the prognosis of breast cancer. A recent study has shown that AI can evaluate a mammography screening in the same level of an average breast radiologist. </p>

<p>In coming years, we can expect to see great developments in Natural Language Processing and AI Analytics. This will broaden the horizons for machine learning capabilities.<br>
In Analytics, the field of AI predictive analyses is projected to receive investments of 11 billion in 2022. The outcome of those investments in projects and studies could be used to improve medical diagnosis, people’s security, read customer behaviour and help us tackle the changing face of climate. </p>

<p>Advancements in Natural Language Processing could let us start reading books tailored to our preferences of number of pages, written style, genre and even name and relationship of characters. And this is due the success of better language modelling such as GPT-2 from OpenAI and the abilities it equips developers with.<br>
Similar algorithms are already being used to create music, for instance MuseNet, also from OpenAI.</p>

<p>We expect to see advance in AI voice assistants that will replace the first line of contact in call centres. AI will be capable to understand context and reply as natural as another human, different from automated recorded messages used today. </p>

<p>Regarding security, more countries could choose to follow China’s and Russia’s implementation of facial recognition to make our cities safer by installing cameras capable of alerting the police when a wanted person is identified on the streets. On the same trend, building entrance cameras could start taking facial recognition to let residents in and out or our cars could have a fingerprint check before unlocking the doors.</p> 

<p>The increase of possible things to do with AI will impact our everyday lives and cities. We think that the more present AI is, the more light is shine upon the discussion of privacy and security of our data. <br>
Cybersecurity threats are currently difficult to be assessed by artificial intelligence and ground-breaking discoveries in this field would lead to new milestones in AI development and applications.</p>

<p>It is clear that the implementation of AI will impact jobs in the next decade. Advanced chatbots to replace call centre attendants is an example. <br>
A Study done by McKinsey Global Institute indicates that by 2030, between 3% and 14% of the global workforce might need to acquire new skills or find a different work.<br>
Living in a time which information is easily accessible will prove to be very beneficial for us during this time of change. We can find prediction of growth and demand for our jobs. </p>

<p>With this data already available, companies should be taking an informed decision to promote further training for their workers and governments to providing more financial incentive towards education and community access to technologies.
This is what will differentiate the AI revolution from the 18th century manufacturing revolution and, if done in a planned manner, every member of our society should benefit from it.</p>

<p> Rodriguez-Ruiz A, Lång K, Gubern-Merida A, et al., Y 2019, ‘Stand-Alone Artificial Intelligence for Breast Cancer Detection in Mammography: Comparison With 101 Radiologists’,
Journal of the National Cancer Institute, vol. 111, pp. 916-922. </p>


<p> <i>Better Language Models and Their Implications</i>, February 2019, viewed 24 April 2020,
<a href="https://openai.com/blog/better-language-models/#sample3"> <https://openai.com/blog/better-language-models/#sample3>
</a> </p>

<p> 
James M, Sussan L, Michael C, Jacques B, Jonathan W, et al., <i> What the future of work will mean for jobs, skills, and wages: Jobs lost, jobs gained,</i> November 2017, McKinsey Global Institute, viewed 24 April 2020,
<a href="https://www.mckinsey.com/featured-insights/future-of-work/jobs-lost-jobs-gained-what-the-future-of-work-will-mean-
for-jobs-skills-and-wages"> < https://www.mckinsey.com/featured-insights/future-of-work/jobs-lost-jobs-gained-what-the-future-of-work-will-mean-for-jobs-skills-and-wages> </a> </p>




<h1> Medical Informatics / Remote Medicine </h1>
<p>Medical informatics is the combination of health care, information science and computer science where it is the intersection of healthcare and technology.  It focuses on how technology collects data to then be used to improve healthcare and the patient’s outcome by using technology to help health the patient. They can use the data collected from patients to then be used to help decision making for hospitality in order to push through research and improve healthcare. The evolution of medical informatics is separated into 5 periods. The first period occurred during 1955-1965 where researches decided to test and experiment technology with the medical field which led to an invention of a biomedical program which helped assisted biostatistics. About 40 million dollars were given to research in this field which allowed technology to rapidly improve. The next period, 1965-1975, near the 70s countries in Europe were using the technology information systems which are installed in computers and processers were upgraded overtime into micro technology. Two researchers, Godfrey N. Hounsfield and Allan M. Cormack received the first noble prize in 1979 for the achievements in the medical informatics field. During the middle period from 1975 – 1985, significant growth in technology which positively affected the health care system as more workers attained more education in this field as the technology has also furthered improved. The technology that has changed during this period was the memory storages as the capacity of information that could be stored expanded the combination of technology in the medical field. During the fourth period, an enhancement in the medical informatic fields occurred as the research became more intense and new standards have been reached and more knowledge were known for workers studying in the field. As technology advanced, medical centers have also increased in capacity and supported most of the newer technology that were introduced which has helped improve the health care around the globe. Up until the final period, technology has simultaneously progressed overtime along with the growth of medical informatics. Further investment in medical informatics allowed growth in technology to be used in the medical field informatics drastically improved the hardware and software technologies which has created high tech computers to be used. As technology is evolving over time and we are finding more ways to improve the technology we have such as computers and the amount of data that can be stored as technology is required to assist and help improve the healthcare system. If we are able to create even faster computers by improving the components in the computer and increase the data storage and reduce the size (example is a hard drive being able to store more data but the size has decreased which can further boost the technology we have nowadays. An improvement in Artificial Intelligence may occur which can create both positive and negative effects on society. The positive effects on medical informatics is that it will be very convenient as it can be easily used, reduce the amount of errors that could have been done by humans and it also accomplish tasks given without having to rely on emotion or stamina apart from where they need to be recharged or replaced. However, some negative effects of AI on medical informatics can be that current cost will be very expensive where healthcare system cannot possibly afford to invest into AI, the AI robots will take away jobs but these robots will not replace jobs as humans possess values such as decision making and emotions as the elderly will prefer to be assisted by a human being rather than a robot as they will not feel lonely which ties in on emotion. 
</p>
<p>The potential impact of research and development in medical informatics field is that it will help boost medical hospitality section such as nursing home, hospitals, etc. As further there is further investment in the medical informatics and remote medicine field, we can help aid patients that need help no matter what type of environment they live in, rural or metropolitan, having equality across humans no matter if you are in a poor or healthy condition. As technology advances, they can receive and transfer information at a rapid rate, which allows nurses and doctors to gain and use the information faster than before. This allows them to use the data gathered and make decisions faster than before in order to help aid the patient in need which will result in patients being able to recover faster or more patients surviving which will boost the effectiveness of healthcare around the globe. As the technology becomes more developed, more places in the medical hospitality can access and use it which will allow better accessibility to those who live in rural areas where this technology may not be provided. Jobs will not be replaced as the technology will require human-to-human interaction to help aid patients as humans will prefer not to be helped by a robot, however technology will be replaced as more investment occurs in this field, places like nursing homes, etc., must invest money into the technology to gather the data from patients and see what is required to done to solve the problem and provide a service to the patient, not robots. The result of further research between the intersection of information technology and medical field can help benefit society as everyone, no matter where you live or what has occurred, can have accessible developed healthcare that can help assist or solve their problems through new technology.
</p>
<p>Personally, I do not get severe injuries that I need to go to the hospital and get it checked out and recover. However, if I do get injured badly, then the new technology can help create a much faster recovery compared to before as technology advances. It will positively impact my family, friends and members in society as everyone in the community has access to these goods and services which will boost recovery and potentially save more patients through brand new technology in the medical field. In my own perspective, I would very happy if this occurs because as technology becomes faster, more convenient and more developed, it can save the some of the population than before. If more research occurs in medical informatics and remote medicine, it can assist workers such as nurses, doctors, surgeons to help know what the problem is occurring to the patient and how it can the patient can be recovered. This type of technology will not cause jobs losses but will ease the pressure off workers in the medical field as the technology speeds up the process of obtaining and analyzing information at a rapid rate which can allow these workers to tend other patients that are in need. In summary, information can be gathered from the patient faster which allows workers in the medical field to analyze the information to create an understanding of what the problem is for the patient and how can it be solved. This will allow more time to be spent on other patients that are in need whereby technology has further advanced in the medical field.
</p>
<p>References:
https://www.usfhealthonline.com/resources/key-concepts/what-is-medical-informatics/
https://healthinformatics.uic.edu/blog/what-is-medical-informatics/
https://jamanetwork.com/journals/jama/article-abstract/380818
https://en.wikipedia.org/wiki/Rural_health
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3947939/
</p>


<h1> Cryptocurrency </h1>
<p>Data encryption is the translation of data into an encoded form, allowing access only to those with a secret key or password. This is an effective method to securely store data, as its appearance becomes unreadable to unwanted entities or people. Currency is a ‘medium of exchange’ issued by the government, allowing us to purchase the things we want and need, additionally, providing the services for a up and running society. Combining the two, we get Cryptocurrency. Thus, cryptocurrency exists as an encrypted form of currency in the digital world, issued by no one. However, this would not be possible without blockchain technology and its ability to decentralise all cryptocurrency.
</p>
<p>Blockchain technology works by transacting data on all computers connected to one network, rather than on a single server. For example, banks operate on a client to server database, where all information is stored in one place. This has its disadvantages, where data can be changed, servers can be hacked, and currency can be transferred. With blockchain technology, data appears on all computers, which cannot be changed or hacked as all network participants need to give permission. How blockchain works is that, for every new transaction in a blockchain, a new block is created, which is then added to the network after being checked by a majority of system nodes, or, computers.
</p>
<p>In the creation of a block, a ‘hash’ is generated thanks to a cryptographic hashing algorithm, enabling the identification of each block in a blockchain structure. The hash assists in detecting change within the block and is the main factor of security in blockchain technology. So, in the event of a hacking attempt, the blocks will have incorrect information thus invalidating the entire blockchain system. This technology makes it impossible to hack the system, as it is the most transparent and secure. Everything is done anonymously, and transactions become irreversible forever. No other technology can do this, which is what makes blockchains so intriguing.
</p>
<p>Since the creation of cryptocurrency and blockchain technology in 2009, its popularity has grown larger than ever in an attempt to solve the ‘double-spending problem’, which has been gaining attention from everyone around the world, especially governments. Since the government controls our currency, there is an underlying threat they face. Due to the heavily secured nature of blockchain technology, officials face the issue  that is, allowing ordinary individuals to have complete control over their money. Because of this, we expect in the near future, involvement of governments and blockchain technology. 
</p>
<p>In the coming years, cryptocurrency will become the new norm as the future advances. New currencies will start emerging and the numbers will continue to rise as more people adopt this new technology. This complex form of currency is revolutionary and can benefit us. Times are changing and the acceptance of that will allow our society to grow bigger and better. However, with the good comes the bad, and this is shown in the mining aspect of cryptocurrency.
</p>
<p>The process of mining cryptocurrency uses a considerable amount of energy. Mining involves solving complex tasks in order to obtain new blocks, which requires extremely powerful hardware. As time passes, the complexity of these tasks increases, resulting in constant hardware upgrades. A concern in this is that cryptocurrency needs a lot of energy to mine, so with a gain of popularity, is a gain in difficulty to mine. Therefore, value increases and more people will use more computing power. There is a lot of energy waste and use. As coal and other fossil fuels are a major source of our electricity, cryptocurrency mining becomes an additional contributor to climate change, threatening the progress towards a low carbon economy. 
</p>
<p>Cryptocurrency and blockchain technology do not necessarily negatively impact people. If anything, it will mostly benefit them. However, as a result of this new adoption, banks and financial industries, including those with a career in these jobs, may become disrupted in the coming years. In order to stay relevant, they will need to introduce this technology into their field. Individuals will begin to open their own cryptocurrency bank accounts, crypto debit cards will become a norm and cryptocurrencies will be withdrawn from ATM’s instead of money. The more people to use this cryptocurrency, the more reason there is for a business in this field to open. Furthermore, transaction processes will become instant, so to stay in brand, the banking and finance industry needs to implement  this new technology. 
</p>
<p>Besides the banking and financing industries, other regular companies and business will likely accept cryptocurrency as a form of payment, but despite its popularity, the general public is not completely aware of this new technology. It will take years to have a fully crypto and blockchain tech future. As of now, regular individuals are able to mine and collect cryptocurrency while it is still worth its valuable. The future is near and as the crypto community grows, society will near a better financial system.
</p>


<h1> Cyber Security </h1>
<p>What does Cyber Security do? Who needs Cyber Security? Why do people need Cyber security? Those are three questions that crucial to ask any idea, technology based or not. Cyber Security is a very broad term that has many use cases and specialities within it. What Cyber Security does it exactly what the name state, it is the security of the internet. Everyone needs Cyber Security, it protects everything from your bank accounts, to people turning on your smart oven in the middle of the night. Cyber Security is in everything and is an extremely important aspect of computing.</p>
<p>When I say that everyone needs Cyber Security, I am simply elaborating on the idea that your data has a price tag and should be protected. Your data is yours; nobody should be able to come in a view it or take it as they please. Yours data should be protected and if your data is comprised you should be notified immediately to act. Cyber Security is used in banks, grocery stores, online commerce, and every form of online computing. Cyber Security is used in many different applications. Most commonly, the basic user will recognize the use of &ldquo;security questions&rdquo; as an authentication to authenticate the user and confirm you are who you say you. This is most used when accessing sensitive materials or attempting to change login information. However, on the back end, which the user can consider &ldquo;behind the scenes&rdquo;, Cyber Security is everywhere. A CAPTCHA is another user recognizable form of Cyber Security. A <em>CAPTCHA</em> is used to distinguish a real, alive person to a robot. <em>CAPTCHA</em> does not say how exactly they do this, because the point is for it is not beaten. That being said, it is confirmed that <em>CAPTCHA</em> uses many variables to help form a decision on if the user is a person or not. These variables can be cursor accuracy, time to complete task, task completion percentage, and the IP address the user is accessing from. Cursor accuracy is commonly used because a robot or basically anything that is not a person will point exactly to where they need to go. A person on the other hand may hover around the page with their cursor before finally executing the task. Even at it&rsquo;s very best, CAPTCHA can still be beaten, or alternatively turn away a human. This creates a frustrating user experience, and nobody wants that. The next step would be a 100% source of authenticate that is hassle-free and cannot be tricked or deceived by another computer. There are many other use cases of Cyber Security however I felt that these were the two most user relatable cases, they are cases that users see and interact with extremely often when browsing online.</p>
<p>Cyber Security has a large impact on we the user, and usually the customer interacts with the internet as we know it. The <em>CAPTCHA&nbsp;</em>development mentioned earlier is a great example of a Cyber Security development that is on-going and creating a much more secure internet. As this technology improves, users should expect their online banking portals and other sensitive data to be much more secure, with no added difficulty for the user. In a perfect scenario the user is uninterrupted in their browsing, and behind the scenes, in the back end of the website, the user is authenticated and granted access. However, I believe that we are still quite a while away from that &ldquo;seamless&rdquo; internet. That task comes with great technological challenges, security challenges, and most importantly legislative challenges. Legislative challenges are when a governing body must approve the use of the technology, this can become a challenge when the technology is brand new and not completely understood by legislators and representatives. I believe that in order to achieve the &ldquo;seamless experience&rdquo; data such as cookies and browsing history would need to be read by the website. This creates an issue with reading people&rsquo;s data without their permission, as touched on early in this report.</p>
<p>In my daily life, in the perfect scenario, my online browsing experience becomes seamless and all authentication happens in the background. This would drastically increase my productivity online when logging in and attempting to view sensitive materials. As of right now, every time I login to my bank accounts online, I must answer security questions or complete a challenge. This task usually takes about 15 seconds and when doing that multiple times, a day, the time really begin to add up. Over the time span of years, this could save me many minutes or even hours allowing me to be more productive with that time saved. Perhaps even finish more university assignments. My family friends that own a concrete cutting business would extremely benefit from a &ldquo;seamless experience&rdquo;. Kimberly, who is the co-owner of the company spends multiple minutes every day when logging into their job calendar, website for bidding on jobs, system that pays employees, and the bank. The time she would save would be many times greater than I would. She could use that time to create more revenue for the company, growing the company quicker, and expanding her business.</p>
